<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://daseinda.github.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://daseinda.github.io//" rel="alternate" type="text/html" hreflang="en" /><updated>2023-10-10T11:11:45+00:00</updated><id>https://daseinda.github.io//feed.xml</id><title type="html">blank</title><subtitle>A lifelong learner interested in the modern world.
</subtitle><entry><title type="html">The pure C implementation for LeNet5</title><link href="https://daseinda.github.io//blog/2023/lenet5-c/" rel="alternate" type="text/html" title="The pure C implementation for LeNet5" /><published>2023-06-20T22:32:51+00:00</published><updated>2023-06-20T22:32:51+00:00</updated><id>https://daseinda.github.io//blog/2023/lenet5-c</id><content type="html" xml:base="https://daseinda.github.io//blog/2023/lenet5-c/"><![CDATA[<h2 id="the-visualization-of-lenet5-structure">The visualization of LeNet5 structure</h2>

<!-- <img src="/assets/img/blogs/2023/LeNet5_C/LeNet5_Structure.jpg"  width="500">  -->

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/assets/img/blogs/2023/LeNet5_C/LeNet5_Structure.jpg" width="500" />
    <br />
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">[2]</div>
</center>

<p>I am recording my LeNet5 C implementation for Intel SGX usage. Because Intel SGX interfaces can only be written in pure C, even C++ would not work. Assume the dataset is MNIST, the size of the black-white image matrix is (1,28,28). The model parameters matrix number group used in the implementation is 3-dimensional. There are 4-dimensional number group in input/output matrices to record the products during the learning process.</p>

<h2 id="how-to-calculate-the-matrix-size-after-layer-operations">How to calculate the matrix size after layer operations:</h2>

<h3 id="convolution">Convolution</h3>
<p>Assumptions:</p>
<ul>
  <li>The size of input matrix: (dimentional,W,W)</li>
  <li>The size of kernel: (kernel_dimentional, F,F)</li>
  <li>The step size: S</li>
  <li>Padding: P</li>
</ul>

\[N = \frac{W-F+2P}{S}+1\]

<p>The size of output matrix size is N.</p>

<h2 id="the-meaning-of-batch-size">The meaning of Batch Size</h2>
<ul>
  <li>Batch size is the number of training images between each backward/parameter updates.</li>
  <li>Batch size training can be parallelized with openmp at C. But not work at Intel SGX up to now(I am trying to apply the openmp in Intel SGX at the present).</li>
</ul>

<h2 id="convolution-implementation">Convolution implementation</h2>
<pre><code class="language-C">    #define CONVOLUTION_FORWARD(input,output,weight,bias,action)					\
    {																				
      for (int x = 0; x &lt; GETLENGTH(weight); ++x)									\
        for (int y = 0; y &lt; GETLENGTH(*weight); ++y)							\
          CONVOLUTE_VALID(input[x], output[y], weight[x][y]);					\
      FOREACH(j, GETLENGTH(output))												\
        FOREACH(i, GETCOUNT(output[j]))											\
        ((double *)output[j])[i] = action(((double *)output[j])[i] + bias[j]);	\
    }
</code></pre>
<ul>
  <li>The input[x] is the channel of the input matrix.</li>
  <li>The output[y] is the number of feature map for input image’s x channel. For each image’s x channel, the first (for) loop plus each x convolution computation operation to the output[y], and achieve the convolution performance.</li>
</ul>

<h2 id="convolution-backward-implementation">Convolution Backward Implementation</h2>
<pre><code class="language-C">  #define CONVOLUTION_BACKWARD(input,inerror,outerror,weight,wd,bd,actiongrad)\
  {																			\
    for (int x = 0; x &lt; GETLENGTH(weight); ++x)								\
      for (int y = 0; y &lt; GETLENGTH(*weight); ++y)						\
        CONVOLUTE_FULL(outerror[y], inerror[x], weight[x][y]);			\
    FOREACH(i, GETCOUNT(inerror))											\
      ((double *)inerror)[i] *= actiongrad(((double *)input)[i]);			\
    FOREACH(j, GETLENGTH(outerror))											\
      FOREACH(i, GETCOUNT(outerror[j]))									\
      bd[j] += ((double *)outerror[j])[i];								\
    for (int x = 0; x &lt; GETLENGTH(weight); ++x)								\
      for (int y = 0; y &lt; GETLENGTH(*weight); ++y)						\
        CONVOLUTE_VALID(input[x], wd[x][y], outerror[y]);				\
  }
</code></pre>
<ul>
  <li>The backward of convolution is indeed the gradient calculation iterated from output layer to input layer, each layer’s gradient depends on the next layer gradient and the layer’s current parameters. From the implementation, we can figure out one shortage of resnet, which originally should make it competitive to Transformer in my opinion. That the res-connection-previous-layer’s gradient in resnet should participate into the res-connection-current-layer’s gradient parameter update on numerial/optimization/mathematical aspect. Due to the limitation of python engineering implementation, such gradient-connection are omitted, and the previous input value simply takes part in the current input. I also would not intend to discuss such missing feature for res-connection at the present thesis work due to the large work-load I have had. But may research it in the future spare time.</li>
  <li>Assume layer $n^{th}$, the kernel size is m*m, each output $y_{i}$ calculation would update the elements kernel $\omega_{1}$ - $\omega_{m^{2}}$. L is the loss.
\(\frac{\partial{L}}{\partial{\omega}}\)</li>
</ul>

<h2 id="pooling-backward">Pooling Backward</h2>

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/assets/img/blogs/2023/LeNet5_C/maxpool_backward.png" width="500" />
    <br />
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Max Pooling Backward Mechanism</div>
</center>

<ul>
  <li>MaxPooling(Sub-sampling Layer) is used in the LeNet5 implementation</li>
</ul>

<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/assets/img/blogs/2023/LeNet5_C/avgpool_backward.jpg" width="500" />
    <br />
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">Average Pooling Backward Mechanism</div>
</center>

<h2 id="preprocessing-load_input">PreProcessing load_input</h2>
<ul>
  <li>Add padding =2 to input matrix, the output matrix is N = 28+2*2=32. Achieve by create a layer0 matrix with required size of 32,32, then fill in image’s data from 3 to 30(number group from 2 to 29)</li>
  <li>
    <p>Standardlization and normalization</p>

    <pre><code class="language-C">   static inline void load_input(Feature *features, image input)
   {
     double (*layer0)[LENGTH_FEATURE0][LENGTH_FEATURE0] = features-&gt;input;
     const long sz = sizeof(image) / sizeof(**input);
     double mean = 0, std = 0;
     FOREACH(j, sizeof(image) / sizeof(*input))
       FOREACH(k, sizeof(*input) / sizeof(**input))
     {
       mean += input[j][k];
       std += input[j][k] * input[j][k];
     }
     mean /= sz;
     std = sqrt(std / sz - mean*mean);
     FOREACH(j, sizeof(image) / sizeof(*input))
       FOREACH(k, sizeof(*input) / sizeof(**input))
     {
       layer0[0][j + PADDING][k + PADDING] = (input[j][k] - mean) / std;
     }
   }
</code></pre>
  </li>
</ul>

<h2 id="initial-the-strcuture-of-lenet5">Initial the strcuture of LeNet5</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>```C
  typedef struct LeNet5
  {
      double weight0_1[INPUT][LAYER1][LENGTH_KERNEL][LENGTH_KERNEL];
      double weight2_3[LAYER2][LAYER3][LENGTH_KERNEL][LENGTH_KERNEL];
      double weight4_5[LAYER4][LAYER5][LENGTH_KERNEL][LENGTH_KERNEL];
      double weight5_6[LAYER5 * LENGTH_FEATURE5 * LENGTH_FEATURE5][OUTPUT];
      /*bias*/
      double bias0_1[LAYER1];
      double bias2_3[LAYER3];
      double bias4_5[LAYER5];
      double bias5_6[OUTPUT];
  }LeNet5;
```
</code></pre></div></div>

<h3 id="the-explanation-for-c-struct-lenet5">The explanation for C struct LeNet5</h3>

<h4 id="c1-layer">C1 Layer</h4>
<ul>
  <li>wegiht0_1: The 1st convolution layer/kernels for the input image(size: 1,32,32), the size of this kernel is: (INPUT=1,LENGTH_KERNEL=5,LENGTH_KERNEL=5), there are LAYER1=6 kernels. The step size is 1. After the first layer, the matrix size would change from (1,28,28) to (Layer1=6, N=28,N=28), consists of 6 feature maps. no padding, padding =0;</li>
  <li>bias0_1: The bias for each kernel at C1 layer, there are LAYER1=6 bias corresponding to each feacture map.</li>
  <li>activation: relu. after each convolution operation. Replace <strong><em>CNN-POOL-RELU</em></strong> with <strong><em>CNN-RELU-POOL</em></strong> process order, the latter one is mostly used in recent cnn applications.
    <h4 id="s2-layer">S2 Layer</h4>
    <p>This layer is pure computing operations, and no weights in the layer to compute. So There are no parameters at this layer for LeNet5 C struct. There is no activation function setting for this implementation</p>
    <h5 id="down-sampling-layer">down-sampling layer</h5>
    <p>The down-sampling kernel size is (2,2). The output at this layer is 28/kernel_size = (6,14,14), consists of 6 feature maps of size(14,14).</p>
  </li>
</ul>

<h4 id="c3-layer">C3 Layer</h4>
<p>This is the third layer of LeNet5, a convolutional layer</p>
<ul>
  <li>weight2_3: LAYER2=6 is the input matrix’s dimentional from S2 Layer. LAYER3=16 is the output matrix’s dimensional from C3 layer. The kernels at this layer is (LAYER3=16, LENGTH_KERNEL = 5,5).</li>
  <li>bias2_3: The bias for the 16 kerners at layer 3.</li>
  <li>Not all feature maps from S2 will be used in C3.</li>
</ul>
<center>
    <img style="border-radius: 0.3125em;
    box-shadow: 0 2px 4px 0 rgba(34,36,38,.12),0 2px 10px 0 rgba(34,36,38,.08);" src="/assets/img/blogs/2023/LeNet5_C/throw_connection_lenet5.PNG" width="500" />
    <br />
    <div style="color:orange; border-bottom: 1px solid #d9d9d9;
    display: inline-block;
    color: #999;
    padding: 2px;">[2] Not all S2's feature maps will be used in C3</div>
</center>

<ul>
  <li><strong><em>The implementation does not implement the above connenction throwing.</em></strong></li>
</ul>

<h2 id="the-data-structure-of-input-and-output-during-learning">The data structure of input and output during learning</h2>
<pre><code class="language-C">  typedef struct Feature
  {
    double input[INPUT][LENGTH_FEATURE0][LENGTH_FEATURE0];
    double layer1[LAYER1][LENGTH_FEATURE1][LENGTH_FEATURE1];
    double layer2[LAYER2][LENGTH_FEATURE2][LENGTH_FEATURE2];
    double layer3[LAYER3][LENGTH_FEATURE3][LENGTH_FEATURE3];
    double layer4[LAYER4][LENGTH_FEATURE4][LENGTH_FEATURE4];
    double layer5[LAYER5][LENGTH_FEATURE5][LENGTH_FEATURE5];
    double output[OUTPUT];
  }Feature;
</code></pre>
<ul>
  <li>The INPUT is the channel of input 2-dimensional matrixs. The input is 3-dimensional matrix.</li>
</ul>

<h2 id="reference">Reference</h2>
<div id="refer-anchor-1"></div>

<ul>
  <li>[1] https://zhuanlan.zhihu.com/p/41736894</li>
</ul>

<div id="refer-anchor-1"></div>

<ul>
  <li>[2] https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=726791</li>
</ul>]]></content><author><name></name></author><category term="CNN_C" /><summary type="html"><![CDATA[The pure C implementation for LeNet5]]></summary></entry></feed>